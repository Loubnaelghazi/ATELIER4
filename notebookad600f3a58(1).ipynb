{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2>Full name : El ghazi Loubna </h2>\n<h1>Part 2: tRANSFORMER TEXT GENERATION </h1>","metadata":{}},{"cell_type":"markdown","source":"***This code first loads a subset of the Wikipedia dataset, tokenizes the text data using the GPT-2 tokenizer, and fine-tunes the GPT-2 model on the tokenized dataset. After fine-tuning, it generates text based on a given prompt using the fine-tuned model.***\n<br/>Make sure to install :<br/>\n!pip install transformers<br/>\n!pip install torch\n","metadata":{}},{"cell_type":"markdown","source":"I imported A dataset from the Hugging Face Datasets library !\nso make sure to install : </br>\n!pip install datasets\n","metadata":{}},{"cell_type":"markdown","source":"I followed this tutoriel : https://gist.github.com/mf1024/3df214d2f17f3dcc56450ddf0d5a4cd7","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\nimport logging\nlogging.getLogger().setLevel(logging.CRITICAL)\n\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'","metadata":{"execution":{"iopub.status.busy":"2024-05-26T12:53:58.340651Z","iopub.execute_input":"2024-05-26T12:53:58.341010Z","iopub.status.idle":"2024-05-26T12:54:05.219187Z","shell.execute_reply.started":"2024-05-26T12:53:58.340984Z","shell.execute_reply":"2024-05-26T12:54:05.218057Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nimport random\n# Loading Wikipedia dataset\nwikipedia_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1000]\",trust_remote_code=True) #with the languge code  so avoiding errors\n# Sample a portion of the articles\nsampled_articles = random.sample(wikipedia_dataset[\"text\"], k=100)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:02:30.746917Z","iopub.execute_input":"2024-05-26T13:02:30.747274Z","iopub.status.idle":"2024-05-26T13:02:32.700709Z","shell.execute_reply.started":"2024-05-26T13:02:30.747246Z","shell.execute_reply":"2024-05-26T13:02:32.699816Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import GPT2Config, AdamW\nimport random\n\n# Initialize GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\n# Preprocess the dataset (ensure all elements are strings)\ntext_dataset = [str(article) for article in wikipedia_dataset[\"text\"]]\n\n# Tokenize the dataset and truncate to the maximum sequence length\nmax_seq_length = 1024\ntokenized_dataset = []\nfor article in text_dataset:\n    truncated_article = article[:max_seq_length] \n    tokens = tokenizer.encode(truncated_article, add_special_tokens=True)\n    tokenized_dataset.append(tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:27:23.579802Z","iopub.execute_input":"2024-05-26T13:27:23.580535Z","iopub.status.idle":"2024-05-26T13:27:27.400301Z","shell.execute_reply.started":"2024-05-26T13:27:23.580500Z","shell.execute_reply":"2024-05-26T13:27:27.399264Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Define GPT-2 model configuration\nmodel_config = GPT2Config.from_pretrained('gpt2')\n\n# Instantiate GPT-2 model\nmodel = GPT2LMHeadModel.from_pretrained('gpt2', config=model_config)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:28:19.773562Z","iopub.execute_input":"2024-05-26T13:28:19.774208Z","iopub.status.idle":"2024-05-26T13:28:20.251890Z","shell.execute_reply.started":"2024-05-26T13:28:19.774171Z","shell.execute_reply":"2024-05-26T13:28:20.251000Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"#fine-tuning parametrs \n\nlearning_rate = 1e-5\nepochs = 3\nbatch_size = 4","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:28:24.926068Z","iopub.execute_input":"2024-05-26T13:28:24.926403Z","iopub.status.idle":"2024-05-26T13:28:24.932661Z","shell.execute_reply.started":"2024-05-26T13:28:24.926377Z","shell.execute_reply":"2024-05-26T13:28:24.930016Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"#optimizer \noptimizer = AdamW(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:28:27.892357Z","iopub.execute_input":"2024-05-26T13:28:27.893248Z","iopub.status.idle":"2024-05-26T13:28:27.911301Z","shell.execute_reply.started":"2024-05-26T13:28:27.893214Z","shell.execute_reply":"2024-05-26T13:28:27.910426Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Fonction de padding pour le DataLoader\ndef collate_fn(batch):\n    max_len = max(len(seq) for seq in batch)\n    padded_batch = []\n    for seq in batch:\n        # Vérifier si la séquence tronquée nécessite du padding\n        if len(seq) < max_len:\n            padded_seq = seq + [tokenizer.pad_token_id] * (max_len - len(seq))\n            padded_batch.append(padded_seq)\n    # Vérifier si la liste padded_batch est vide\n    if not padded_batch:\n        print(\"All sequences are already of maximum length. Skipping batch.\")\n        return None\n    print(f\"Length of padded_batch: {len(padded_batch)}\")\n    return torch.tensor(padded_batch)\n\n# DataLoader\ntrain_loader = DataLoader(tokenized_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:36:57.682229Z","iopub.execute_input":"2024-05-26T13:36:57.682652Z","iopub.status.idle":"2024-05-26T13:36:57.690707Z","shell.execute_reply.started":"2024-05-26T13:36:57.682620Z","shell.execute_reply":"2024-05-26T13:36:57.689832Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# Fine-tuning the model\nmodel.train()\nfor epoch in range(epochs):\n    total_loss = 0\n    for batch in train_loader:\n        input_ids = batch.clone().detach()\n        labels = batch.clone().detach()\n        labels[labels == tokenizer.pad_token_id] = -100  # Ignorer la perte pour les tokens de padding\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generate text :","metadata":{}},{"cell_type":"code","source":"def generate_text(model, tokenizer, prompt, max_length=100, temperature=1.0):\n    # Encode the prompt\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n    \n    # Generate text with attention mask and setting pad token ID to eos token ID\n    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n    model.config.pad_token_id = model.config.eos_token_id\n    output_ids = model.generate(input_ids, attention_mask=attention_mask, do_sample=True, max_length=max_length, temperature=temperature)\n    \n    # Decode and return the generated text\n    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return generated_text\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:42:28.666049Z","iopub.execute_input":"2024-05-26T13:42:28.666424Z","iopub.status.idle":"2024-05-26T13:42:28.672899Z","shell.execute_reply.started":"2024-05-26T13:42:28.666395Z","shell.execute_reply":"2024-05-26T13:42:28.671812Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"#exemple :\nprompt = \"Depression is \"","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:51:08.013610Z","iopub.execute_input":"2024-05-26T13:51:08.014213Z","iopub.status.idle":"2024-05-26T13:51:08.018427Z","shell.execute_reply.started":"2024-05-26T13:51:08.014184Z","shell.execute_reply":"2024-05-26T13:51:08.017449Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"#output :\n# Generate text based on the prompt\ngenerated_text = generate_text(model, tokenizer, prompt)\nprint(\"Generated Text:\")\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:51:09.579317Z","iopub.execute_input":"2024-05-26T13:51:09.580205Z","iopub.status.idle":"2024-05-26T13:51:15.213604Z","shell.execute_reply.started":"2024-05-26T13:51:09.580173Z","shell.execute_reply":"2024-05-26T13:51:15.212663Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Text:\nDepression is  saying, \" That doesn't make me happy, but, that makes me happy?\" \"I'm happy  that I felt as good as any.\" \"Well. what do I feel when I think about how I felt if I was happy with this?\" \"Well \" \"It's not the thing I feel for.\" \" \" that makes me happy.\" \"It makes me even more happy.\" \"I feel. I. I feel.\" \"It make [says to myself] that feel?\" \"I have feelings, I feel.\"\" \"That makes me happy.\" He said, \" You know.\" He said, \" If your life is worth living, you don't have. You can\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndevice ","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:42:59.970085Z","iopub.execute_input":"2024-05-26T13:42:59.970475Z","iopub.status.idle":"2024-05-26T13:42:59.976790Z","shell.execute_reply.started":"2024-05-26T13:42:59.970447Z","shell.execute_reply":"2024-05-26T13:42:59.975768Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}